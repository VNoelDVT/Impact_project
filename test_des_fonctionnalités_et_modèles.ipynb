{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "import rouge_score\n",
    "import sqlite3\n",
    "from contextlib import contextmanager\n",
    "from DataMigration.database_functions import create_tables, log_to_sqlite\n",
    "\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QWEN2.5-0.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Loading and Initialization\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "Qwen_25_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "Qwen_25 = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# GPU Check\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Move model to GPU and set training mode\n",
    "Qwen_25 = Qwen_25.to(device)\n",
    "Qwen_25.train()  # Enable dropout in inference phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inputs(text, dropout_prob=0, num_samples=1):\n",
    "    # Paramétrage du taux de dropout pour tous les modules Dropout du modèle\n",
    "    for module in Qwen_25.modules():\n",
    "        if isinstance(module, torch.nn.Dropout):\n",
    "            module.p = dropout_prob\n",
    "\n",
    "    # Fonction pour calculer l'entropie\n",
    "    def calculate_entropy(probabilities):\n",
    "        entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-12), dim=-1)\n",
    "        return entropy\n",
    "    \n",
    "    inputs = Qwen_25_tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        embeddings = Qwen_25.get_input_embeddings()(inputs['input_ids'])\n",
    "    \n",
    "    responses = []\n",
    "    token_entropies_list = []\n",
    "    avg_token_entropies_list = []\n",
    "    top_logits_list = []\n",
    "    top_tokens_list = []\n",
    "    top_probabilities_list = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        with torch.no_grad():\n",
    "            output = Qwen_25.generate(**inputs, return_dict_in_generate=True, output_scores=True, do_sample=True, max_new_tokens=700)\n",
    "            generated_text = Qwen_25_tokenizer.decode(output.sequences[0], skip_special_tokens=True).replace(text, '').strip()\n",
    "            responses.append(generated_text)\n",
    "                \n",
    "            logits = torch.stack(output.scores, dim=0)  # Logits des tokens générés\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Calculer l'entropie des tokens\n",
    "            token_entropies = calculate_entropy(probabilities)\n",
    "            avg_token_entropy = token_entropies.mean().item()\n",
    "            \n",
    "            # Obtenir les indices et logits des top 10 pour le dernier token\n",
    "            top_logits, top_indices = torch.topk(logits[-1], k=10, dim=-1)\n",
    "            mask = top_logits >= 1e-3  # Filtrer les logits faibles\n",
    "            filtered_logits = top_logits[mask]\n",
    "            filtered_indices = top_indices[mask]\n",
    "            \n",
    "            filtered_probabilities = torch.softmax(filtered_logits, dim=-1) if len(filtered_logits) > 0 else []\n",
    "\n",
    "            top_tokens = [Qwen_25_tokenizer.decode(idx) for idx in filtered_indices.tolist()]\n",
    "            \n",
    "            # Stocker les informations dans les listes\n",
    "            token_entropies_list.append(token_entropies.cpu().numpy())\n",
    "            avg_token_entropies_list.append(avg_token_entropy)\n",
    "            top_logits_list.append(filtered_logits.cpu().numpy())\n",
    "            top_tokens_list.append(top_tokens)\n",
    "            top_probabilities_list.append(filtered_probabilities.cpu().numpy())\n",
    "    \n",
    "    # Assembler les données dans un DataFrame pour ce texte\n",
    "    results_df = pd.DataFrame({\n",
    "        'original_text': [text] * num_samples,\n",
    "        'generated_response': responses,\n",
    "        'embeddings': [embeddings.cpu().numpy()] * num_samples,\n",
    "        'token_entropies': token_entropies_list,\n",
    "        'avg_token_entropy': avg_token_entropies_list,\n",
    "        'top_10_logits': top_logits_list,\n",
    "        'top_10_tokens': top_tokens_list,\n",
    "        'top_10_probabilities': top_probabilities_list\n",
    "    })\n",
    "    log_to_sqlite(\"process_input_table\", results_df)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QWEN2.5-1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Loading and Initialization\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "Qwen_25_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "Qwen_25 = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# GPU Check\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Move model to GPU and set training mode\n",
    "Qwen_25 = Qwen_25.to(device)\n",
    "Qwen_25.train()  # Enable dropout in inference phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inputs(text, dropout_prob=0, num_samples=1):\n",
    "    # Paramétrage du taux de dropout pour tous les modules Dropout du modèle\n",
    "    for module in Qwen_25.modules():\n",
    "        if isinstance(module, torch.nn.Dropout):\n",
    "            module.p = dropout_prob\n",
    "\n",
    "    # Fonction pour calculer l'entropie\n",
    "    def calculate_entropy(probabilities):\n",
    "        entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-12), dim=-1)\n",
    "        return entropy\n",
    "    \n",
    "    inputs = Qwen_25_tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        embeddings = Qwen_25.get_input_embeddings()(inputs['input_ids'])\n",
    "    \n",
    "    responses = []\n",
    "    token_entropies_list = []\n",
    "    avg_token_entropies_list = []\n",
    "    top_logits_list = []\n",
    "    top_tokens_list = []\n",
    "    top_probabilities_list = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        with torch.no_grad():\n",
    "            output = Qwen_25.generate(**inputs, return_dict_in_generate=True, output_scores=True, do_sample=True, max_new_tokens=700)\n",
    "            generated_text = Qwen_25_tokenizer.decode(output.sequences[0], skip_special_tokens=True).replace(text, '').strip()\n",
    "            responses.append(generated_text)\n",
    "                \n",
    "            logits = torch.stack(output.scores, dim=0)  # Logits des tokens générés\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Calculer l'entropie des tokens\n",
    "            token_entropies = calculate_entropy(probabilities)\n",
    "            avg_token_entropy = token_entropies.mean().item()\n",
    "            \n",
    "            # Obtenir les indices et logits des top 10 pour le dernier token\n",
    "            top_logits, top_indices = torch.topk(logits[-1], k=10, dim=-1)\n",
    "            mask = top_logits >= 1e-3  # Filtrer les logits faibles\n",
    "            filtered_logits = top_logits[mask]\n",
    "            filtered_indices = top_indices[mask]\n",
    "            \n",
    "            filtered_probabilities = torch.softmax(filtered_logits, dim=-1) if len(filtered_logits) > 0 else []\n",
    "\n",
    "            top_tokens = [Qwen_25_tokenizer.decode(idx) for idx in filtered_indices.tolist()]\n",
    "            \n",
    "            # Stocker les informations dans les listes\n",
    "            token_entropies_list.append(token_entropies.cpu().numpy())\n",
    "            avg_token_entropies_list.append(avg_token_entropy)\n",
    "            top_logits_list.append(filtered_logits.cpu().numpy())\n",
    "            top_tokens_list.append(top_tokens)\n",
    "            top_probabilities_list.append(filtered_probabilities.cpu().numpy())\n",
    "    \n",
    "    # Assembler les données dans un DataFrame pour ce texte\n",
    "    results_df = pd.DataFrame({\n",
    "        'original_text': [text] * num_samples,\n",
    "        'generated_response': responses,\n",
    "        'embeddings': [embeddings.cpu().numpy()] * num_samples,\n",
    "        'token_entropies': token_entropies_list,\n",
    "        'avg_token_entropy': avg_token_entropies_list,\n",
    "        'top_10_logits': top_logits_list,\n",
    "        'top_10_tokens': top_tokens_list,\n",
    "        'top_10_probabilities': top_probabilities_list\n",
    "    })\n",
    "    log_to_sqlite(\"process_input_table\", results_df)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"c'est quoi le méchanisme de fast attention\"\n",
    "output = process_inputs(input)\n",
    "print(output[\"generated_response\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QWEN2.5-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Loading and Initialization\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "Qwen_25_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "Qwen_25 = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# GPU Check\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Move model to GPU and set training mode\n",
    "Qwen_25 = Qwen_25.to(device)\n",
    "Qwen_25.train()  # Enable dropout in inference phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inputs(text, dropout_prob=0, num_samples=1):\n",
    "    # Paramétrage du taux de dropout pour tous les modules Dropout du modèle\n",
    "    for module in Qwen_25.modules():\n",
    "        if isinstance(module, torch.nn.Dropout):\n",
    "            module.p = dropout_prob\n",
    "\n",
    "    # Fonction pour calculer l'entropie\n",
    "    def calculate_entropy(probabilities):\n",
    "        entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-12), dim=-1)\n",
    "        return entropy\n",
    "    \n",
    "    inputs = Qwen_25_tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        embeddings = Qwen_25.get_input_embeddings()(inputs['input_ids'])\n",
    "    \n",
    "    responses = []\n",
    "    token_entropies_list = []\n",
    "    avg_token_entropies_list = []\n",
    "    top_logits_list = []\n",
    "    top_tokens_list = []\n",
    "    top_probabilities_list = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        with torch.no_grad():\n",
    "            output = Qwen_25.generate(**inputs, return_dict_in_generate=True, output_scores=True, do_sample=True, max_new_tokens=700)\n",
    "            generated_text = Qwen_25_tokenizer.decode(output.sequences[0], skip_special_tokens=True).replace(text, '').strip()\n",
    "            responses.append(generated_text)\n",
    "                \n",
    "            logits = torch.stack(output.scores, dim=0)  # Logits des tokens générés\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Calculer l'entropie des tokens\n",
    "            token_entropies = calculate_entropy(probabilities)\n",
    "            avg_token_entropy = token_entropies.mean().item()\n",
    "            \n",
    "            # Obtenir les indices et logits des top 10 pour le dernier token\n",
    "            top_logits, top_indices = torch.topk(logits[-1], k=10, dim=-1)\n",
    "            mask = top_logits >= 1e-3  # Filtrer les logits faibles\n",
    "            filtered_logits = top_logits[mask]\n",
    "            filtered_indices = top_indices[mask]\n",
    "            \n",
    "            filtered_probabilities = torch.softmax(filtered_logits, dim=-1) if len(filtered_logits) > 0 else []\n",
    "\n",
    "            top_tokens = [Qwen_25_tokenizer.decode(idx) for idx in filtered_indices.tolist()]\n",
    "            \n",
    "            # Stocker les informations dans les listes\n",
    "            token_entropies_list.append(token_entropies.cpu().numpy())\n",
    "            avg_token_entropies_list.append(avg_token_entropy)\n",
    "            top_logits_list.append(filtered_logits.cpu().numpy())\n",
    "            top_tokens_list.append(top_tokens)\n",
    "            top_probabilities_list.append(filtered_probabilities.cpu().numpy())\n",
    "    \n",
    "    # Assembler les données dans un DataFrame pour ce texte\n",
    "    results_df = pd.DataFrame({\n",
    "        'original_text': [text] * num_samples,\n",
    "        'generated_response': responses,\n",
    "        'embeddings': [embeddings.cpu().numpy()] * num_samples,\n",
    "        'token_entropies': token_entropies_list,\n",
    "        'avg_token_entropy': avg_token_entropies_list,\n",
    "        'top_10_logits': top_logits_list,\n",
    "        'top_10_tokens': top_tokens_list,\n",
    "        'top_10_probabilities': top_probabilities_list\n",
    "    })\n",
    "    log_to_sqlite(\"process_input_table\", results_df)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"c'est quoi le méchanisme de fast attention\"\n",
    "output = process_inputs(input)\n",
    "print(output[\"generated_response\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
